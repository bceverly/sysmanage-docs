<!DOCTYPE html>
<html lang="en" data-i18n-title="testing.title">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title data-i18n="testing.title" data-i18n-html>Testing - SysManage Server</title>
    <meta name="description" content="Comprehensive testing guide for SysManage including unit tests, integration tests, and E2E testing with Playwright." data-i18n-attr="content" data-i18n="testing.description" data-i18n-html>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="icon" type="image/svg+xml" href="/assets/images/favicon.svg">
</head>
<body data-auto-header="documentation" data-auto-footer>
    <!-- Header injected automatically by components.js -->

    <main class="docs-main">
        <div class="container">
            <div class="docs-breadcrumb">
                <a href="../" data-i18n="nav.documentation" data-i18n-html>Documentation</a> > <a href="./" data-i18n="server_docs.section_title" data-i18n-html>Server</a> > <span data-i18n="testing.breadcrumb" data-i18n-html>Testing</span>
            </div>

            <div class="docs-header">
                <h1 data-i18n="testing.title" data-i18n-html>Testing</h1>
                <p data-i18n="testing.subtitle" data-i18n-html>Comprehensive testing strategy for SysManage server including unit tests, integration tests, and end-to-end testing with Playwright.</p>
            </div>

            <div class="docs-content">
                <section class="docs-overview">
                    <h2 data-i18n="testing.overview.title" data-i18n-html>Testing Strategy Overview</h2>
                    <p data-i18n="testing.overview.description" data-i18n-html>SysManage employs a multi-layered testing approach to ensure reliability, security, and functionality across all components:</p>

                    <div class="testing-layers">
                        <div class="testing-layer">
                            <h3>üß™ <span data-i18n="testing.overview.unit.title" data-i18n-html>Unit Tests</span></h3>
                            <p data-i18n="testing.overview.unit.description" data-i18n-html>Comprehensive unit testing for individual components and functions</p>
                            <ul>
                                <li><strong data-i18n="testing.overview.unit.backend.text" data-i18n-html>Backend:</strong> <span data-i18n="testing.overview.unit.backend.count" data-i18n-html>1,432 Python tests using pytest</span></li>
                                <li><strong data-i18n="testing.overview.unit.frontend.text" data-i18n-html>Frontend:</strong> <span data-i18n="testing.overview.unit.frontend.count" data-i18n-html>63 TypeScript tests using Vitest</span></li>
                                <li><strong data-i18n="testing.overview.unit.coverage.text" data-i18n-html>Coverage:</strong> <span data-i18n="testing.overview.unit.coverage.value" data-i18n-html>100% test coverage for both backend and frontend</span></li>
                            </ul>
                        </div>
                        <div class="testing-layer">
                            <h3>üîó <span data-i18n="testing.overview.integration.title" data-i18n-html>Integration Tests</span></h3>
                            <p data-i18n="testing.overview.integration.description" data-i18n-html>Testing interactions between different system components</p>
                            <ul>
                                <li data-i18n="testing.overview.integration.api" data-i18n-html>API endpoint testing</li>
                                <li data-i18n="testing.overview.integration.database" data-i18n-html>Database integration testing</li>
                                <li data-i18n="testing.overview.integration.websocket" data-i18n-html>WebSocket communication testing</li>
                                <li data-i18n="testing.overview.integration.auth" data-i18n-html>Authentication and authorization flow testing</li>
                            </ul>
                        </div>
                        <div class="testing-layer">
                            <h3>üé≠ <span data-i18n="testing.overview.e2e.title" data-i18n-html>End-to-End (E2E) Tests</span></h3>
                            <p data-i18n="testing.overview.e2e.description" data-i18n-html>Complete user workflow testing using Playwright</p>
                            <ul>
                                <li data-i18n="testing.overview.e2e.journey" data-i18n-html>Full user journey testing</li>
                                <li data-i18n="testing.overview.e2e.crossbrowser" data-i18n-html>Cross-browser compatibility</li>
                                <li data-i18n="testing.overview.e2e.realworld" data-i18n-html>Real-world scenario validation</li>
                                <li data-i18n="testing.overview.e2e.ui" data-i18n-html>UI interaction and workflow testing</li>
                            </ul>
                        </div>
                        <div class="testing-layer">
                            <h3>üéØ <span data-i18n="testing.overview.msw.title" data-i18n-html>Mock Service Worker (MSW)</span></h3>
                            <p data-i18n="testing.overview.msw.description" data-i18n-html>API mocking for isolated and reliable frontend testing</p>
                            <ul>
                                <li data-i18n="testing.overview.msw.isolation" data-i18n-html>Complete test isolation from backend dependencies</li>
                                <li data-i18n="testing.overview.msw.speed" data-i18n-html>Faster test execution without network requests</li>
                                <li data-i18n="testing.overview.msw.reliability" data-i18n-html>Consistent and predictable test behavior</li>
                                <li data-i18n="testing.overview.msw.realistic" data-i18n-html>Realistic API response simulation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section class="docs-section">
                    <h2 data-i18n="testing.running.title" data-i18n-html>Running Tests</h2>

                    <h3 data-i18n="testing.running.all.title" data-i18n-html>All Tests</h3>
                    <div class="code-block">
                        <pre><code># Run complete test suite
make test

# Run tests with coverage reporting
make test-coverage</code></pre>
                    </div>

                    <h3 data-i18n="testing.running.backend.title" data-i18n-html>Backend Tests (Python/pytest)</h3>
                    <div class="code-block">
                        <pre><code># Run all backend tests
python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_api_hosts.py -v

# Run with coverage
python -m pytest tests/ --cov=backend --cov-report=html

# Run only unit tests
python -m pytest tests/ -m "not integration"

# Run only integration tests
python -m pytest tests/ -m integration</code></pre>
                    </div>

                    <h3 data-i18n="testing.running.frontend.title" data-i18n-html>Frontend Tests (TypeScript/Vitest with MSW)</h3>
                    <p data-i18n="testing.running.frontend.description" data-i18n-html>Frontend tests automatically use Mock Service Worker for API mocking:</p>
                    <div class="code-block">
                        <pre><code># Run all frontend tests (with MSW automatically active)
cd frontend && npm test

# Run tests in watch mode
cd frontend && npm run test:watch

# Run with coverage
cd frontend && npm run test:coverage

# Run specific test file
cd frontend && npm test -- src/__tests__/Pages/Reports.test.tsx

# View MSW request logs in tests
cd frontend && npm test -- --reporter=verbose</code></pre>
                    </div>

                    <div class="info-box">
                        <h4>üéØ <span data-i18n="testing.running.frontend.msw.title" data-i18n-html>MSW Behavior</span></h4>
                        <p data-i18n="testing.running.frontend.msw.description" data-i18n-html>When running frontend tests:</p>
                        <ul>
                            <li data-i18n="testing.running.frontend.msw.automatic" data-i18n-html>MSW automatically intercepts all API requests</li>
                            <li data-i18n="testing.running.frontend.msw.logging" data-i18n-html>Request logs appear in console output</li>
                            <li data-i18n="testing.running.frontend.msw.isolation" data-i18n-html>Each test runs in complete isolation</li>
                            <li data-i18n="testing.running.frontend.msw.realistic" data-i18n-html>Realistic API responses are provided automatically</li>
                        </ul>
                    </div>

                    <h3 data-i18n="testing.running.e2e.title" data-i18n-html>End-to-End Tests (Playwright)</h3>
                    <div class="code-block">
                        <pre><code># Install Playwright (if not already installed)
pip install playwright
npx playwright install

# Run E2E tests
npm run test:e2e

# Run E2E tests in specific browser
npx playwright test --project=chromium
npx playwright test --project=firefox
npx playwright test --project=webkit

# Run E2E tests in headed mode (visible browser)
npx playwright test --headed

# Debug E2E tests
npx playwright test --debug</code></pre>
                    </div>
                </section>

                <section class="docs-section">
                    <h2 data-i18n="testing.msw.title" data-i18n-html>Mock Service Worker (MSW) Frontend Testing</h2>

                    <h3 data-i18n="testing.msw.overview.title" data-i18n-html>Overview</h3>
                    <p data-i18n="testing.msw.overview.description" data-i18n-html>Mock Service Worker (MSW) is a powerful API mocking library that intercepts HTTP requests at the network level, providing realistic API responses for frontend testing. SysManage uses MSW extensively to ensure frontend components can be tested in isolation without requiring a running backend server.</p>

                    <div class="info-box">
                        <h4>üí° <span data-i18n="testing.msw.benefits.title" data-i18n-html>Why MSW?</span></h4>
                        <ul>
                            <li data-i18n="testing.msw.benefits.isolation" data-i18n-html><strong>Test Isolation:</strong> Frontend tests run independently of backend availability</li>
                            <li data-i18n="testing.msw.benefits.speed" data-i18n-html><strong>Performance:</strong> No network latency, tests execute faster</li>
                            <li data-i18n="testing.msw.benefits.reliability" data-i18n-html><strong>Reliability:</strong> Consistent responses eliminate flaky tests</li>
                            <li data-i18n="testing.msw.benefits.development" data-i18n-html><strong>Development:</strong> Work on frontend features before backend APIs are ready</li>
                            <li data-i18n="testing.msw.benefits.cicd" data-i18n-html><strong>CI/CD:</strong> No need to spin up backend services in test environments</li>
                        </ul>
                    </div>

                    <h3 data-i18n="testing.msw.installation.title" data-i18n-html>Installation & Configuration</h3>
                    <p data-i18n="testing.msw.installation.description" data-i18n-html>MSW is already installed and configured in the SysManage frontend. The setup includes:</p>

                    <div class="code-block">
                        <pre><code># MSW installation
npm install --save-dev msw@^2.11.3

# MSW configuration in package.json
{
  "msw": {
    "workerDirectory": ["public"]
  }
}</code></pre>
                    </div>

                    <h3 data-i18n="testing.msw.implementation.title" data-i18n-html>Implementation Architecture</h3>

                    <h4 data-i18n="testing.msw.handlers.title" data-i18n-html>Request Handlers</h4>
                    <p data-i18n="testing.msw.handlers.description" data-i18n-html>MSW handlers are defined in <code>/src/mocks/handlers.ts</code> and provide comprehensive API coverage:</p>

                    <div class="code-block">
                        <pre><code>// /src/mocks/handlers.ts
import { http, HttpResponse } from 'msw';

export const handlers = [
  // Catch all /api/ requests and handle them dynamically
  http.get('http://localhost:8080/api/*', ({ request }) => {
    const url = new globalThis.URL(request.url);
    const path = url.pathname;

    // Host data endpoint
    if (path === '/api/host/1' || path === '/api/hosts/1') {
      return HttpResponse.json({
        id: 1,
        fqdn: 'test-host.example.com',
        ipv4: '192.168.1.100',
        active: true,
        status: 'up',
        platform: 'Linux',
        // ... complete host data
      });
    }

    // Package search with query parameter handling
    if (path === '/api/packages/search') {
      const query = url.searchParams.get('query');
      if (!query || query.length < 2) {
        return HttpResponse.json([]);
      }

      const results = availablePackages.filter(pkg =>
        pkg.name.toLowerCase().includes(query.toLowerCase())
      );
      return HttpResponse.json(results);
    }

    // Default response for unmatched endpoints
    return HttpResponse.json([]);
  }),

  // POST endpoints for package management
  http.post('http://localhost:8080/api/packages/install/*', async ({ request }) => {
    const body = await request.json();
    return HttpResponse.json({
      success: true,
      message: 'Package installation has been queued',
      installation_ids: body.package_names.map(() => `uuid-${Math.random()}`)
    });
  }),
];</code></pre>
                    </div>

                    <h4 data-i18n="testing.msw.server.title" data-i18n-html>Test Server Setup</h4>
                    <p data-i18n="testing.msw.server.description" data-i18n-html>The MSW server is configured for Node.js test environments in <code>/src/mocks/node.ts</code>:</p>

                    <div class="code-block">
                        <pre><code>// /src/mocks/node.ts
import { setupServer } from 'msw/node';
import { handlers } from './handlers';

// Setup MSW server for Node.js environment (tests)
export const server = setupServer(...handlers);</code></pre>
                    </div>

                    <h4 data-i18n="testing.msw.integration.title" data-i18n-html>Test Integration</h4>
                    <p data-i18n="testing.msw.integration.description" data-i18n-html>MSW is automatically configured for all tests in <code>/src/setupTests.ts</code> with proper lifecycle management:</p>

                    <div class="code-block">
                        <pre><code>// /src/setupTests.ts
import { beforeAll, afterEach, afterAll } from 'vitest';
import { server } from './mocks/node';

// Start MSW server before all tests
beforeAll(() => {
  server.listen({
    onUnhandledRequest: 'warn', // Warn about unhandled requests
  });

  if (process.env.CI === 'true') {
    console.log('üöÄ MSW server started for CI environment');
  }
});

// Reset handlers after each test for isolation
afterEach(() => {
  server.resetHandlers();
});

// Clean up after all tests
afterAll(() => {
  server.close();
});</code></pre>
                    </div>

                    <h3 data-i18n="testing.msw.features.title" data-i18n-html>Key Features</h3>

                    <h4 data-i18n="testing.msw.features.dynamic.title" data-i18n-html>Dynamic Path Handling</h4>
                    <p data-i18n="testing.msw.features.dynamic.description" data-i18n-html>MSW uses broad pattern matching with dynamic path resolution:</p>
                    <ul>
                        <li data-i18n="testing.msw.features.dynamic.wildcard" data-i18n-html>Wildcard patterns catch all <code>/api/*</code> requests</li>
                        <li data-i18n="testing.msw.features.dynamic.parsing" data-i18n-html>URL parsing extracts specific endpoint paths</li>
                        <li data-i18n="testing.msw.features.dynamic.fallback" data-i18n-html>Graceful fallback for unmatched endpoints</li>
                    </ul>

                    <h4 data-i18n="testing.msw.features.realistic.title" data-i18n-html>Realistic Response Simulation</h4>
                    <p data-i18n="testing.msw.features.realistic.description" data-i18n-html>MSW provides comprehensive mock data that mirrors real API responses:</p>
                    <ul>
                        <li data-i18n="testing.msw.features.realistic.hosts" data-i18n-html>Complete host information with hardware details</li>
                        <li data-i18n="testing.msw.features.realistic.users" data-i18n-html>User authentication and profile data</li>
                        <li data-i18n="testing.msw.features.realistic.packages" data-i18n-html>Software package data with search functionality</li>
                        <li data-i18n="testing.msw.features.realistic.operations" data-i18n-html>Package installation/uninstallation operations</li>
                    </ul>

                    <h4 data-i18n="testing.msw.features.ci.title" data-i18n-html>CI/CD Integration</h4>
                    <p data-i18n="testing.msw.features.ci.description" data-i18n-html>MSW includes special handling for continuous integration environments:</p>
                    <div class="code-block">
                        <pre><code>// Enhanced logging for CI environments
const logPrefix = process.env.CI === 'true' ? 'üîÑ MSW-CI:' : 'MSW:';
console.log(`${logPrefix} Handling GET ${path}`);

// CI-specific startup and shutdown messages
if (process.env.CI === 'true') {
  console.log('üöÄ MSW server started for CI environment');
}</code></pre>
                    </div>

                    <h3 data-i18n="testing.msw.usage.title" data-i18n-html>Usage in Tests</h3>

                    <h4 data-i18n="testing.msw.usage.automatic.title" data-i18n-html>Automatic API Mocking</h4>
                    <p data-i18n="testing.msw.usage.automatic.description" data-i18n-html>Most tests automatically benefit from MSW without additional configuration:</p>
                    <div class="code-block">
                        <pre><code>// Test automatically uses MSW for API calls
test('renders host information', async () => {
  render(<HostDetail hostId={1} />);

  // MSW automatically returns mock host data
  await waitFor(() => {
    expect(screen.getByText('test-host.example.com')).toBeInTheDocument();
    expect(screen.getByText('192.168.1.100')).toBeInTheDocument();
  });
});</code></pre>
                    </div>

                    <h4 data-i18n="testing.msw.usage.custom.title" data-i18n-html>Custom Handler Overrides</h4>
                    <p data-i18n="testing.msw.usage.custom.description" data-i18n-html>Tests can override specific handlers for custom scenarios:</p>
                    <div class="code-block">
                        <pre><code>import { server } from '../../mocks/node';
import { http, HttpResponse } from 'msw';

test('handles API error gracefully', async () => {
  // Override handler for this test
  server.use(
    http.get('http://localhost:8080/api/host/1', () => {
      return HttpResponse.json(
        { error: 'Host not found' },
        { status: 404 }
      );
    })
  );

  render(<HostDetail hostId={1} />);

  await waitFor(() => {
    expect(screen.getByText('Error loading host')).toBeInTheDocument();
  });
});</code></pre>
                    </div>

                    <h3 data-i18n="testing.msw.compatibility.title" data-i18n-html>React 19 Compatibility</h3>
                    <p data-i18n="testing.msw.compatibility.description" data-i18n-html>SysManage includes special compatibility fixes for React 19 in the test environment:</p>
                    <div class="code-block">
                        <pre><code>// React 19 compatibility fixes in setupTests.ts
globalThis.IS_REACT_ACT_ENVIRONMENT = true;

// Scheduler polyfills for React 19
Object.defineProperty(globalThis, 'Scheduler', {
  value: {
    unstable_scheduleCallback: (callback) => setTimeout(callback, 0),
    unstable_cancelCallback: () => {},
    unstable_shouldYield: () => false,
    // ... additional React 19 compatibility
  }
});</code></pre>
                    </div>

                    <h3 data-i18n="testing.msw.bestpractices.title" data-i18n-html>Best Practices</h3>
                    <ul>
                        <li data-i18n="testing.msw.bestpractices.isolation" data-i18n-html><strong>Test Isolation:</strong> MSW automatically resets handlers between tests</li>
                        <li data-i18n="testing.msw.bestpractices.realistic" data-i18n-html><strong>Realistic Data:</strong> Use mock data that closely matches production API responses</li>
                        <li data-i18n="testing.msw.bestpractices.coverage" data-i18n-html><strong>API Coverage:</strong> Ensure all API endpoints used by components are mocked</li>
                        <li data-i18n="testing.msw.bestpractices.errors" data-i18n-html><strong>Error Scenarios:</strong> Test both success and error responses</li>
                        <li data-i18n="testing.msw.bestpractices.logging" data-i18n-html><strong>Request Logging:</strong> Use MSW logging to debug unexpected API calls</li>
                    </ul>

                    <div class="info-box">
                        <h4>üîç <span data-i18n="testing.msw.debugging.title" data-i18n-html>Debugging MSW</span></h4>
                        <p data-i18n="testing.msw.debugging.description" data-i18n-html>If tests fail due to API issues, check:</p>
                        <ul>
                            <li data-i18n="testing.msw.debugging.console" data-i18n-html>Console output for MSW request logs</li>
                            <li data-i18n="testing.msw.debugging.unhandled" data-i18n-html>Warnings about unhandled requests</li>
                            <li data-i18n="testing.msw.debugging.handlers" data-i18n-html>Handler patterns match actual API calls</li>
                            <li data-i18n="testing.msw.debugging.setup" data-i18n-html>MSW server is properly started in test setup</li>
                        </ul>
                    </div>
                </section>

                <section class="docs-section">
                    <h2>Playwright E2E Testing</h2>

                    <h3>Overview</h3>
                    <p>Playwright provides comprehensive end-to-end testing capabilities for SysManage, allowing you to test complete user workflows across different browsers and platforms.</p>

                    <h3>Installation & Setup</h3>
                    <div class="code-block">
                        <pre><code># Install Playwright
npm install -D @playwright/test

# Install browser binaries
npx playwright install

# Install system dependencies (Linux)
npx playwright install-deps</code></pre>
                    </div>

                    <div class="info-box">
                        <h4>üí° Platform Compatibility</h4>
                        <p>Playwright may not be available on all platforms. For example, it's currently not supported on OpenBSD. In such cases, manual testing or alternative tools should be used.</p>
                    </div>

                    <h3>Configuration</h3>
                    <p>Playwright configuration is defined in <code>playwright.config.ts</code>:</p>
                    <div class="code-block">
                        <pre><code>import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './e2e',
  fullyParallel: true,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 0,
  workers: process.env.CI ? 1 : undefined,
  reporter: 'html',
  use: {
    baseURL: 'http://localhost:3000',
    trace: 'on-first-retry',
    screenshot: 'only-on-failure',
  },
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
    },
    {
      name: 'firefox',
      use: { ...devices['Desktop Firefox'] },
    },
    {
      name: 'webkit',
      use: { ...devices['Desktop Safari'] },
    },
  ],
  webServer: {
    command: 'npm run start',
    url: 'http://localhost:3000',
    reuseExistingServer: !process.env.CI,
  },
});</code></pre>
                    </div>

                    <h3>Example E2E Test</h3>
                    <div class="code-block">
                        <pre><code>// e2e/login-workflow.spec.ts
import { test, expect } from '@playwright/test';

test('user can login and access dashboard', async ({ page }) => {
  // Navigate to login page
  await page.goto('/login');

  // Fill login form
  await page.fill('[data-testid=username]', 'admin@example.com');
  await page.fill('[data-testid=password]', 'password123');

  // Submit form
  await page.click('[data-testid=login-button]');

  // Verify successful login
  await expect(page).toHaveURL('/dashboard');
  await expect(page.locator('h1')).toContainText('Dashboard');
});

test('user can generate reports', async ({ page }) => {
  // Login first
  await page.goto('/login');
  await page.fill('[data-testid=username]', 'admin@example.com');
  await page.fill('[data-testid=password]', 'password123');
  await page.click('[data-testid=login-button]');

  // Navigate to reports
  await page.click('text=Reports');
  await expect(page).toHaveURL('/reports');

  // Generate PDF report
  await page.click('text=Generate PDF');

  // Wait for download
  const downloadPromise = page.waitForEvent('download');
  const download = await downloadPromise;

  // Verify download
  expect(download.suggestedFilename()).toContain('.pdf');
});</code></pre>
                    </div>

                    <h3>Best Practices</h3>
                    <ul>
                        <li><strong>Data Attributes:</strong> Use <code>data-testid</code> attributes for reliable element selection</li>
                        <li><strong>Page Object Model:</strong> Organize tests using page objects for maintainability</li>
                        <li><strong>Test Isolation:</strong> Ensure tests are independent and can run in any order</li>
                        <li><strong>Wait Strategies:</strong> Use appropriate wait strategies for dynamic content</li>
                        <li><strong>Screenshots:</strong> Enable screenshots on failure for debugging</li>
                        <li><strong>Cross-browser:</strong> Run tests across multiple browsers to ensure compatibility</li>
                    </ul>
                </section>

                <section class="docs-section">
                    <h2>Test Coverage</h2>

                    <h3>Coverage Reports</h3>
                    <p>SysManage maintains comprehensive test coverage across all layers:</p>

                    <div class="coverage-stats">
                        <div class="coverage-item">
                            <h4>Backend Coverage</h4>
                            <div class="coverage-bar">
                                <div class="coverage-fill" style="width: 100%;">100%</div>
                            </div>
                            <p>1,432 Python tests covering API endpoints, business logic, and database operations</p>
                        </div>
                        <div class="coverage-item">
                            <h4>Frontend Coverage</h4>
                            <div class="coverage-bar">
                                <div class="coverage-fill" style="width: 100%;">100%</div>
                            </div>
                            <p>63 TypeScript tests covering React components, services, and utilities</p>
                        </div>
                    </div>

                    <h3>Generating Coverage Reports</h3>
                    <div class="code-block">
                        <pre><code># Backend coverage (HTML report)
python -m pytest tests/ --cov=backend --cov-report=html
open htmlcov/index.html

# Frontend coverage
cd frontend && npm run test:coverage
open coverage/index.html

# Combined coverage reporting
make test-coverage</code></pre>
                    </div>
                </section>

                <section class="docs-section">
                    <h2>Continuous Integration</h2>

                    <h3>GitHub Actions</h3>
                    <p>All tests run automatically in the CI/CD pipeline:</p>
                    <div class="code-block">
                        <pre><code>name: CI/CD Pipeline
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          cd frontend && npm install

      - name: Run backend tests
        run: python -m pytest tests/ --cov=backend

      - name: Run frontend tests
        run: cd frontend && npm test

      - name: Install Playwright
        run: npx playwright install --with-deps

      - name: Run E2E tests
        run: npm run test:e2e</code></pre>
                    </div>
                </section>

                <section class="docs-section">
                    <h2>Testing Different Components</h2>

                    <h3>API Endpoints</h3>
                    <p>Test API endpoints using pytest with FastAPI's test client:</p>
                    <div class="code-block">
                        <pre><code>def test_get_hosts(authenticated_client):
    response = authenticated_client.get("/api/hosts")
    assert response.status_code == 200
    assert "hosts" in response.json()

def test_create_host(authenticated_client):
    host_data = {
        "fqdn": "test.example.com",
        "active": True
    }
    response = authenticated_client.post("/api/hosts", json=host_data)
    assert response.status_code == 201</code></pre>
                    </div>

                    <h3>React Components</h3>
                    <p>Test React components using Vitest and React Testing Library:</p>
                    <div class="code-block">
                        <pre><code>import { render, screen, fireEvent } from '@testing-library/react';
import { vi } from 'vitest';
import Reports from '../Pages/Reports';

test('generates PDF when button clicked', async () => {
  const mockApi = vi.mocked(api);
  mockApi.get.mockResolvedValue({
    data: new Blob(['pdf'], { type: 'application/pdf' })
  });

  render(<Reports />);

  const generateButton = screen.getByText('Generate PDF');
  fireEvent.click(generateButton);

  await waitFor(() => {
    expect(mockApi.get).toHaveBeenCalledWith(
      '/api/reports/generate/hosts',
      expect.objectContaining({ responseType: 'blob' })
    );
  });
});</code></pre>
                    </div>

                    <h3>WebSocket Communication</h3>
                    <p>Test WebSocket connections and message handling:</p>
                    <div class="code-block">
                        <pre><code>def test_websocket_connection(client):
    with client.websocket_connect("/ws") as websocket:
        websocket.send_text("ping")
        data = websocket.receive_text()
        assert data == "pong"</code></pre>
                    </div>
                </section>

                <section class="docs-section">
                    <h2>Troubleshooting Tests</h2>

                    <h3>Common Issues</h3>

                    <div class="troubleshooting-item">
                        <h4>Playwright Installation Fails</h4>
                        <p><strong>Problem:</strong> Browser binaries fail to install</p>
                        <p><strong>Solution:</strong> Install system dependencies: <code>npx playwright install-deps</code></p>
                    </div>

                    <div class="troubleshooting-item">
                        <h4>Tests Fail in CI but Pass Locally</h4>
                        <p><strong>Problem:</strong> Environment differences between local and CI</p>
                        <p><strong>Solution:</strong> Check for timing issues, use proper wait strategies, ensure consistent test data</p>
                    </div>

                    <div class="troubleshooting-item">
                        <h4>Frontend Tests Can't Find Elements</h4>
                        <p><strong>Problem:</strong> React components not rendering properly in tests</p>
                        <p><strong>Solution:</strong> Use <code>await waitFor()</code> for async operations, check component mocking</p>
                    </div>

                    <div class="troubleshooting-item">
                        <h4>Database Tests Interfere with Each Other</h4>
                        <p><strong>Problem:</strong> Test database state affects other tests</p>
                        <p><strong>Solution:</strong> Use database transactions and rollback, or isolated test databases</p>
                    </div>

                    <h3>Debug Mode</h3>
                    <div class="code-block">
                        <pre><code># Debug backend tests
python -m pytest tests/test_specific.py -v -s --pdb

# Debug frontend tests
cd frontend && npm test -- --run=false

# Debug Playwright tests
npx playwright test --debug</code></pre>
                    </div>
                </section>

                <section class="docs-section">
                    <h2>Performance Testing</h2>

                    <p><strong>Performance testing is now fully integrated into SysManage's test suite</strong> and runs automatically as part of our CI/CD pipeline.</p>

                    <h3>üöÄ Integrated Performance Testing</h3>
                    <div class="info-box">
                        <h4>‚úÖ Current Implementation</h4>
                        <p>SysManage includes comprehensive performance testing that runs on every commit:</p>
                        <ul>
                            <li><strong>Artillery:</strong> Backend API load testing with realistic scenarios</li>
                            <li><strong>Playwright Performance:</strong> Frontend Core Web Vitals monitoring</li>
                            <li><strong>Multi-Platform:</strong> Tested across Linux, macOS, and Windows</li>
                            <li><strong>CI/CD Integration:</strong> Automated performance regression detection</li>
                        </ul>
                    </div>

                    <h3>Running Performance Tests</h3>
                    <div class="code-block">
                        <pre><code># Run comprehensive performance test suite
make test-performance

# Run individual performance tests
make test-artillery      # Backend API load testing
make test-ui-performance # Frontend performance testing

# Run specific Playwright performance tests
pytest tests/ui/test_performance_playwright.py -v</code></pre>
                    </div>

                    <h3>Performance Test Coverage</h3>
                    <ul>
                        <li><strong>Backend Load Testing:</strong> API endpoints, authentication flows, WebSocket connections</li>
                        <li><strong>Frontend Performance:</strong> Page load times, Core Web Vitals, memory usage</li>
                        <li><strong>Network Analysis:</strong> Resource loading optimization and request monitoring</li>
                        <li><strong>Cross-Browser Testing:</strong> Performance validation across Chrome, Firefox, and Safari</li>
                    </ul>

                    <h3>Performance Budgets</h3>
                    <div class="info-box">
                        <h4>üìä Enterprise Performance Standards</h4>
                        <ul>
                            <li><strong>API Response Time:</strong> 95th percentile < 500ms, 99th percentile < 1000ms</li>
                            <li><strong>First Contentful Paint:</strong> < 2000ms</li>
                            <li><strong>DOM Content Loaded:</strong> < 1500ms</li>
                            <li><strong>Error Rate:</strong> < 1%</li>
                            <li><strong>Minimum Throughput:</strong> 8 requests per second</li>
                        </ul>
                    </div>

                    <h3>üìà Performance Documentation</h3>
                    <p>For detailed information about our performance testing framework, see the <a href="../architecture/performance-testing.html">Performance Testing Architecture</a> documentation.</p>
                </section>

                <!-- Cross-Platform UI Testing Framework Section -->
                <section class="docs-section">
                    <h2 data-i18n="ui_testing.title" data-i18n-html>Cross-Platform UI Testing Framework</h2>
                    <p data-i18n="ui_testing.description" data-i18n-html>Revolutionary triple testing framework featuring Playwright (primary), Selenium (fallback), and comprehensive CI/CD integration for cross-platform UI testing excellence.</p>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.overview.title" data-i18n-html>Framework Overview</h3>
                        <p data-i18n="ui_testing.overview.description" data-i18n-html>SysManage implements a revolutionary triple testing framework that automatically adapts to platform capabilities, ensuring comprehensive UI test coverage across all supported environments.</p>

                        <div class="feature-grid">
                            <div class="feature-item">
                                <h4 data-i18n="ui_testing.overview.architecture.title" data-i18n-html>üèóÔ∏è Triple Testing Architecture</h4>
                                <ul>
                                    <li data-i18n="ui_testing.overview.architecture.playwright_primary" data-i18n-html>Playwright (Primary): Linux, macOS, Windows - Full cross-browser support with Chrome, Firefox, and WebKit</li>
                                    <li data-i18n="ui_testing.overview.architecture.selenium_fallback" data-i18n-html>Selenium (Fallback): OpenBSD, FreeBSD - Reliable WebDriver automation where Playwright is unavailable</li>
                                    <li data-i18n="ui_testing.overview.architecture.cicd_integration" data-i18n-html>CI/CD Integration: GitHub Actions with fail-fast logic and comprehensive error reporting</li>
                                </ul>
                            </div>
                            <div class="feature-item">
                                <h4 data-i18n="ui_testing.overview.benefits.title" data-i18n-html>Key Benefits</h4>
                                <ul>
                                    <li data-i18n="ui_testing.overview.benefits.cross_platform" data-i18n-html>Cross-Platform Excellence: Seamless testing across 5 operating systems</li>
                                    <li data-i18n="ui_testing.overview.benefits.cross_browser" data-i18n-html>Cross-Browser Compatibility: Chrome, Firefox, Safari/WebKit support</li>
                                    <li data-i18n="ui_testing.overview.benefits.intelligent_selection" data-i18n-html>Intelligent Selection: Framework automatically chooses optimal testing tool per platform</li>
                                    <li data-i18n="ui_testing.overview.benefits.production_parity" data-i18n-html>Production Parity: Real database integration with proper user lifecycle management</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.playwright.title" data-i18n-html>Playwright Testing (Primary Framework)</h3>
                        <p data-i18n="ui_testing.playwright.description" data-i18n-html>Modern browser automation framework providing cross-browser testing with enterprise-grade reliability.</p>

                        <h4 data-i18n="ui_testing.playwright.features.title" data-i18n-html>üé≠ Playwright Features</h4>
                        <ul>
                            <li data-i18n="ui_testing.playwright.features.cross_browser" data-i18n-html>Cross-Browser Testing: Chromium, Firefox, WebKit (Safari) support</li>
                            <li data-i18n="ui_testing.playwright.features.async_native" data-i18n-html>Async Native: Built for modern JavaScript with native async/await support</li>
                            <li data-i18n="ui_testing.playwright.features.auto_wait" data-i18n-html>Auto-Wait: Intelligent waiting for elements without explicit sleeps</li>
                            <li data-i18n="ui_testing.playwright.features.network_interception" data-i18n-html>Network Interception: Mock and monitor network requests</li>
                            <li data-i18n="ui_testing.playwright.features.screenshots" data-i18n-html>Automatic Screenshots: Failure debugging with visual evidence</li>
                        </ul>

                        <h4 data-i18n="ui_testing.playwright.browser_support.title" data-i18n-html>Browser Support Matrix</h4>
                        <div class="platform-matrix">
                            <div class="platform-row">
                                <span data-i18n="ui_testing.playwright.browser_support.chromium" data-i18n-html>Chromium: Full support on all platforms (Linux, macOS, Windows)</span>
                            </div>
                            <div class="platform-row">
                                <span data-i18n="ui_testing.playwright.browser_support.firefox" data-i18n-html>Firefox: Full support on all platforms</span>
                            </div>
                            <div class="platform-row">
                                <span data-i18n="ui_testing.playwright.browser_support.webkit" data-i18n-html>WebKit (Safari): macOS only - automatically excluded on other platforms</span>
                            </div>
                        </div>

                        <div class="code-block">
                            <pre><code># Run Playwright tests
make test-ui-playwright

# Run cross-browser tests
make test-ui-cross-browser

# Run with specific browser
npx playwright test --project=chromium
npx playwright test --project=firefox
npx playwright test --project=webkit  # macOS only</code></pre>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.selenium.title" data-i18n-html>Selenium Testing (Fallback Framework)</h3>
                        <p data-i18n="ui_testing.selenium.description" data-i18n-html>Reliable WebDriver automation framework serving as intelligent fallback for platforms where Playwright is unavailable.</p>

                        <h4 data-i18n="ui_testing.selenium.use_cases.title" data-i18n-html>üîÑ When Selenium is Used</h4>
                        <ul>
                            <li data-i18n="ui_testing.selenium.use_cases.openbsd" data-i18n-html>OpenBSD: Primary testing framework (Playwright unavailable)</li>
                            <li data-i18n="ui_testing.selenium.use_cases.freebsd" data-i18n-html>FreeBSD: Primary testing framework (Playwright unavailable)</li>
                            <li data-i18n="ui_testing.selenium.use_cases.legacy_systems" data-i18n-html>Legacy Systems: Broader compatibility with older browsers</li>
                        </ul>

                        <div class="code-block">
                            <pre><code># Run Selenium tests (automatic fallback on OpenBSD/FreeBSD)
make test-ui-selenium

# Force Selenium usage
TEST_FRAMEWORK=selenium make test-ui</code></pre>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.database_integration.title" data-i18n-html>Database Integration</h3>
                        <p data-i18n="ui_testing.database_integration.description" data-i18n-html>Sophisticated database integration ensuring tests run against real PostgreSQL with proper user lifecycle management.</p>

                        <h4 data-i18n="ui_testing.database_integration.user_injection.title" data-i18n-html>üîê Test User Injection</h4>
                        <ul>
                            <li data-i18n="ui_testing.database_integration.user_injection.argon2_hashing" data-i18n-html>Argon2 Password Hashing: Production-grade password security with configurable salt</li>
                            <li data-i18n="ui_testing.database_integration.user_injection.yaml_configuration" data-i18n-html>YAML Configuration: Salt and database credentials from sysmanage.yaml</li>
                            <li data-i18n="ui_testing.database_integration.user_injection.unique_users" data-i18n-html>Unique Test Users: Each test run creates isolated test users</li>
                            <li data-i18n="ui_testing.database_integration.user_injection.proper_cleanup" data-i18n-html>Proper Cleanup: Test users automatically removed after test completion</li>
                        </ul>

                        <div class="info-box">
                            <h5>Database Configuration</h5>
                            <p>UI tests use the same PostgreSQL database as the main application, ensuring perfect production parity. Test users are created with proper Argon2 hashing and automatically cleaned up.</p>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.cross_platform.title" data-i18n-html>Cross-Platform Testing Strategy</h3>
                        <p data-i18n="ui_testing.cross_platform.description" data-i18n-html>Comprehensive testing strategy ensuring consistent behavior across all supported operating systems.</p>

                        <h4 data-i18n="ui_testing.cross_platform.platform_matrix.title" data-i18n-html>üìä Platform Testing Matrix</h4>
                        <div class="platform-matrix">
                            <div class="platform-row">
                                <strong>Linux:</strong> <span data-i18n="ui_testing.cross_platform.platform_matrix.linux" data-i18n-html>Playwright with full browser support (Chrome, Firefox, WebKit excluded)</span>
                            </div>
                            <div class="platform-row">
                                <strong>macOS:</strong> <span data-i18n="ui_testing.cross_platform.platform_matrix.macos" data-i18n-html>Playwright with complete browser support (Chrome, Firefox, WebKit)</span>
                            </div>
                            <div class="platform-row">
                                <strong>Windows:</strong> <span data-i18n="ui_testing.cross_platform.platform_matrix.windows" data-i18n-html>Playwright with full browser support (Chrome, Firefox, WebKit excluded)</span>
                            </div>
                            <div class="platform-row">
                                <strong>OpenBSD:</strong> <span data-i18n="ui_testing.cross_platform.platform_matrix.openbsd" data-i18n-html>Selenium with Chrome/Firefox support</span>
                            </div>
                            <div class="platform-row">
                                <strong>FreeBSD:</strong> <span data-i18n="ui_testing.cross_platform.platform_matrix.freebsd" data-i18n-html>Selenium with Chrome/Firefox support</span>
                            </div>
                            <div class="platform-row">
                                <strong>NetBSD:</strong> <span data-i18n="ui_testing.cross_platform.platform_matrix.netbsd" data-i18n-html>Selenium with Chrome-only support (Firefox excluded due to WebDriver compatibility issues)</span>
                            </div>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.cicd_integration.title" data-i18n-html>CI/CD Integration</h3>
                        <p data-i18n="ui_testing.cicd_integration.description" data-i18n-html>Enterprise-grade CI/CD integration with GitHub Actions providing automated testing on every commit and pull request.</p>

                        <h4 data-i18n="ui_testing.cicd_integration.github_actions.title" data-i18n-html>üöÄ GitHub Actions Workflow</h4>
                        <ul>
                            <li data-i18n="ui_testing.cicd_integration.github_actions.fail_fast" data-i18n-html>Fail-Fast Logic: Stop execution immediately on first test failure</li>
                            <li data-i18n="ui_testing.cicd_integration.github_actions.parallel_execution" data-i18n-html>Parallel Execution: Backend, Frontend, and UI tests run concurrently</li>
                            <li data-i18n="ui_testing.cicd_integration.github_actions.comprehensive_logging" data-i18n-html>Comprehensive Logging: Detailed output for debugging failed tests</li>
                            <li data-i18n="ui_testing.cicd_integration.github_actions.artifact_collection" data-i18n-html>Artifact Collection: Screenshots and logs preserved for analysis</li>
                        </ul>

                        <div class="code-block">
                            <pre><code># GitHub Actions workflow automatically runs:
1. Backend Tests (Python/pytest)
2. Frontend Tests (TypeScript/Vitest)
3. UI Tests (Playwright/Selenium)
4. Generate test reports and badges</code></pre>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.makefile_integration.title" data-i18n-html>Makefile Integration</h3>
                        <p data-i18n="ui_testing.makefile_integration.description" data-i18n-html>Seamless integration with existing build system providing simple commands for complex testing operations.</p>

                        <h4 data-i18n="ui_testing.makefile_integration.commands.title" data-i18n-html>üîß Available Commands</h4>
                        <div class="code-block">
                            <pre><code># <span data-i18n="ui_testing.makefile_integration.commands.test_ui" data-i18n-html>Run all UI tests with intelligent framework selection</span>
make test-ui

# <span data-i18n="ui_testing.makefile_integration.commands.test_playwright" data-i18n-html>Force Playwright execution (if available)</span>
make test-playwright

# <span data-i18n="ui_testing.makefile_integration.commands.test_selenium" data-i18n-html>Force Selenium execution</span>
make test-selenium

# <span data-i18n="ui_testing.makefile_integration.commands.test_all" data-i18n-html>Run complete test suite (Backend + Frontend + UI)</span>
make test

# <span data-i18n="ui_testing.makefile_integration.commands.install_browsers" data-i18n-html>Install required browser dependencies</span>
make install-browsers</code></pre>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.debugging.title" data-i18n-html>Debugging and Troubleshooting</h3>
                        <p data-i18n="ui_testing.debugging.description" data-i18n-html>Comprehensive debugging capabilities ensuring rapid issue resolution and test reliability.</p>

                        <h4 data-i18n="ui_testing.debugging.screenshot_capture.title" data-i18n-html>üì∏ Screenshot Capture</h4>
                        <ul>
                            <li data-i18n="ui_testing.debugging.screenshot_capture.automatic" data-i18n-html>Automatic Capture: Screenshots taken on test failure</li>
                            <li data-i18n="ui_testing.debugging.screenshot_capture.timestamped" data-i18n-html>Timestamped Files: Screenshots saved with timestamp for analysis</li>
                            <li data-i18n="ui_testing.debugging.screenshot_capture.storage_location" data-i18n-html>Storage Location: Screenshots saved to /tmp/sysmanage-screenshots/ for examination</li>
                        </ul>

                        <h4 data-i18n="ui_testing.debugging.common_issues.title" data-i18n-html>Common Issues and Solutions</h4>
                        <div class="troubleshooting-grid">
                            <div class="issue-item">
                                <strong data-i18n="ui_testing.debugging.common_issues.browser_not_found" data-i18n-html>Browser Not Found:</strong>
                                <span>Install missing browsers with make install-browsers</span>
                            </div>
                            <div class="issue-item">
                                <strong data-i18n="ui_testing.debugging.common_issues.timeout_errors" data-i18n-html>Timeout Errors:</strong>
                                <span>Increase wait times for slow-loading elements</span>
                            </div>
                            <div class="issue-item">
                                <strong data-i18n="ui_testing.debugging.common_issues.database_connection" data-i18n-html>Database Connection:</strong>
                                <span>Verify PostgreSQL is running and accessible</span>
                            </div>
                        </div>
                    </div>

                    <div class="docs-subsection">
                        <h3 data-i18n="ui_testing.best_practices.title" data-i18n-html>Best Practices</h3>
                        <p data-i18n="ui_testing.best_practices.description" data-i18n-html>Industry best practices for maintainable and reliable UI testing.</p>

                        <h4 data-i18n="ui_testing.best_practices.test_design.title" data-i18n-html>üéØ Test Design Principles</h4>
                        <ul>
                            <li data-i18n="ui_testing.best_practices.test_design.page_object" data-i18n-html>Page Object Pattern: Encapsulate page interactions in reusable objects</li>
                            <li data-i18n="ui_testing.best_practices.test_design.data_driven" data-i18n-html>Data-Driven Tests: Parameterized tests for comprehensive scenario coverage</li>
                            <li data-i18n="ui_testing.best_practices.test_design.independent">Test Independence: Each test runs independently without dependencies</li>
                            <li data-i18n="ui_testing.best_practices.test_design.idempotent">Idempotent Tests: Tests can run multiple times with consistent results</li>
                        </ul>
                    </div>
                </section>
            </div>
        </div>
    </main>
    <!-- Footer injected automatically by components.js -->    <script src="/assets/js/components.js"></script>
    <script src="/assets/js/i18n.js"></script>
    <script src="/assets/js/main.js"></script>
    <script src="/assets/js/navbar.js"></script>
</body>
</html>